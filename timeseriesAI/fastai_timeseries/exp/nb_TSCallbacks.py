#AUTOGENERATED! DO NOT EDIT! file to edit: ./TSCallbacks.ipynb (unless otherwise specified)
try: from exp.nb_TSBasicData import TSItem
except ImportError: from .nb_TSBasicData import TSItem


from fastai.vision import *
from fastai.torch_core import *
from fastai.callback import *
from fastai.callbacks.mixup import *
from fastai.basic_train import Learner, LearnerCallback

device = 'cuda' if torch.cuda.is_available() else 'cpu'


class CutMixCallback(LearnerCallback):

    def __init__(self, learn:Learner, alpha:float=1., alpha2:float=0., stack_y:bool=True,
                 out:bool=False, mix:bool=False):

        super().__init__(learn)
        self.alpha,self.alpha2,self.stack_y,self.out,self.mix = alpha,alpha2,stack_y,out,mix
        assert out + mix < 2, 'cutmix out and mix cannot be selected at the same time'
        assert alpha2 >= 0 and alpha2 <=1, 'alpha2 must be between 0. and 1.'
        if out: self.__name__ = 'CutOutCallBack'
        elif mix: self.__name__ = 'CutMixUpCallBack'
        else: self.__name__ = 'CutMixCallBack'

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies cutmix to `last_input` and `last_target` if `train`."
        if not train or self.alpha == 0: return
        λ = np.random.beta(self.alpha, self.alpha)
        λ = max(λ, 1- λ)
        if self.mix:
            if self.alpha2 == 0: self.alpha2 = self.alpha
            λ2 = np.random.beta(self.alpha2, self.alpha2)
            λ2 = λ + (1 - λ) * λ2
            λ = λ / λ2

        idx = torch.randperm(last_target.size(0)).to(last_input.device)

        # Create new input
        last_input_size = last_input.size()
        bbx1, bby1, bbx2, bby2 = rand_bbox(last_input_size, λ)
        new_input = last_input.clone()
        if self.out: new_input[..., bby1:bby2, bbx1:bbx2] = 0
        elif self.mix: new_input[..., bby1:bby2, bbx1:bbx2] = λ2 * last_input[..., bby1:bby2, bbx1:bbx2] + \
            (1 - λ2) * last_input[idx][..., bby1:bby2, bbx1:bbx2]
        else: new_input[..., bby1:bby2, bbx1:bbx2] = last_input[idx][..., bby1:bby2, bbx1:bbx2]
        λ = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (last_input_size[-1] * last_input_size[-2]))
        if self.mix: λ = λ * λ2
        λ = last_input.new([λ])

        # Modify last target
        if self.stack_y:
            new_target = torch.cat([last_target.unsqueeze(1).float(), last_target[idx].unsqueeze(1).float(),
                                    λ.repeat(last_input_size[0]).unsqueeze(1).float()], 1)
        else:
            if len(last_target.shape) == 2:
                λ = λ.unsqueeze(1).float()
            new_target = last_target.float() * λ + last_target[idx].float() * (1-λ)

        return {'last_input': new_input, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()



def rand_bbox(last_input_size, λ):
    '''lambd is always between .5 and 1'''

    W = last_input_size[-1]
    H = last_input_size[-2]
    cut_rat = np.sqrt(1. - λ) # 0. - .707
    cut_w = np.int(W * cut_rat)
    cut_h = np.int(H * cut_rat)

    # uniform
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    if len(last_input_size) == 4:
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bby2 = np.clip(cy + cut_h // 2, 0, H)
    else:
        bby1 = 0
        bby2 = last_input_size[1]

    return bbx1, bby1, bbx2, bby2


def cutmix(learn:Learner, alpha:float=1., stack_y:bool=True) -> Learner:
    "Add cutmix https://arxiv.org/pdf/1905.04899 to `learn`."
    learn.callback_fns.append(partial(CutMixCallback, alpha=alpha, stack_y=stack_y))
    return learn

def cutout(learn:Learner, alpha:float=1., stack_y:bool=True, out:bool=True) -> Learner:
    "Add cutout https://arxiv.org/abs/1708.04552 to `learn`."
    learn.callback_fns.append(partial(CutMixCallback, alpha=alpha, stack_y=stack_y, out=out))
    return learn

def cutmixup(learn:Learner, alpha:float=1., alpha2:float=1., stack_y:bool=True, mix:bool=True) -> Learner:
    "Add cutmixup to `learn`."
    learn.callback_fns.append(partial(CutMixCallback, alpha=alpha, alpha2=alpha2, stack_y=stack_y, mix=mix))
    return learn

setattr(cutmix, 'cb_fn', CutMixCallback)
Learner.cutmix = cutmix

setattr(cutout, 'cb_fn', CutMixCallback)
Learner.cutout = cutout

setattr(cutmixup, 'cb_fn', CutMixCallback)
Learner.cutmixup = cutmixup

from fastai.train import mixup
setattr(mixup, 'cb_fn', MixUpCallback)


class RicapLoss(nn.Module):
    "Adapt the loss function `crit` to go with ricap data augmentations."

    def __init__(self, crit, reduction='mean'):
        super().__init__()
        if hasattr(crit, 'reduction'):
            self.crit = crit
            self.old_red = crit.reduction
            setattr(self.crit, 'reduction', 'none')
        else:
            self.crit = partial(crit, reduction='none')
            self.old_crit = crit
        self.reduction = reduction

    def forward(self, output, target):
        if target.ndim == 2:
            c_ = target[:, 1:5]
            W_ = target[:, 5:]
            loss = [W_[:, k] * self.crit(output, c_[:, k].long()) for k in range(4)]
            d = torch.mean(torch.stack(loss))
        else: d = self.crit(output, target)
        if self.reduction == 'mean': return d.mean()
        elif self.reduction == 'sum': return d.sum()
        return d

    def get_old(self):
        if hasattr(self, 'old_crit'): return self.old_crit
        elif hasattr(self, 'old_red'):
            setattr(self.crit, 'reduction', self.old_red)
            return self.crit

class RicapCallback(LearnerCallback):
    '''Adapted from :
    paper: https://arxiv.org/abs/1811.09030
    github: https://github.com/4uiiurz1/pytorch-ricap
    and mixup in the fastai library.'''
    def __init__(self, learn:Learner, beta:float=.3, stack_y:bool=True):
        super().__init__(learn)
        self.beta,self.stack_y = beta,stack_y

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = RicapLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies ricap to `last_input` and `last_target` if `train`."
        if not train or self.beta == 0: return

        # get the image size
        I_x, I_y = last_input.size()[2:]

        # draw a boundary position (w, h)
        w = int(np.round(I_x * np.random.beta(self.beta, self.beta)))
        h = int(np.round(I_y * np.random.beta(self.beta, self.beta)))
        w_ = [w, I_x - w, w, I_x - w]
        h_ = [h, h, I_y - h, I_y - h]

        # select and crop four images
        cropped_images = {}
        bs = last_input.size(0)
        c_ = torch.zeros((bs, 4)).float().to(last_input.device)
        W_ = torch.zeros(4).float().to(last_input.device)
        for k in range(4):
            idx = torch.randperm(bs).to(last_input.device)
            x_k = np.random.randint(0, I_x - w_[k] + 1)
            y_k = np.random.randint(0, I_y - h_[k] + 1)
            cropped_images[k] = last_input[idx][:, :, x_k:x_k + w_[k], y_k:y_k + h_[k]]
            c_[:, k] = last_target[idx].float()
            W_[k] = w_[k] * h_[k] / (I_x * I_y)

        # patch cropped images
        patched_images = torch.cat(
            (torch.cat((cropped_images[0], cropped_images[1]), 2),
             torch.cat((cropped_images[2], cropped_images[3]), 2)), 3).to(last_input.device)

        # modify last target
        if self.stack_y:
                new_target = torch.cat((last_target[:,None].float(), c_,
                                        W_[None].repeat(last_target.size(0), 1)), dim=1)
        else:
            new_target = c_ * W_

        return {'last_input': patched_images, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()


def ricap(learn:Learner, beta:float=.3, stack_y:bool=True) -> Learner:
    "Add ricap https://arxiv.org/pdf/1811.09030.pdf to `learn`."
    learn.callback_fns.append(partial(RicapCallback, beta=beta, stack_y=stack_y))
    return learn

setattr(ricap, 'cb_fn', RicapCallback)
Learner.ricap = ricap


def get_fn(a):
    while True:
        if hasattr(a, 'func'): a = a.func
        else: break
    return a

def show_tfms(learn, rows=3, cols=3, figsize=(8, 8)):
    xb, yb = learn.data.one_batch()
    xb = xb.to('cpu')
    yb = yb.to('cpu')
    rand_int = np.random.randint(len(xb))
    ndim = xb.ndim
    tfms = learn.data.train_ds.tfms

    if ndim == 4:
        rand_item = Image(xb[rand_int])
        #print('\noriginal image:')
        #display(rand_item)
        for i in range(len(xb)): xb[i] = Image(xb[i]).apply_tfms(tfms).data.to(device)
        cb_tfms = 0
        for cb in learn.callback_fns:
            if get_fn(cb).__name__ == 'Recorder': continue
            if hasattr(cb, 'keywords') and hasattr(get_fn(cb), 'on_batch_begin'):
                cb_fn = partial(get_fn(cb), **cb.keywords)
                try:
                    fig = plt.subplots(rows, cols, figsize=figsize, sharex=True, sharey=True)[1].flatten()
                    plt.suptitle(get_fn(cb).__name__, cb.keywords, size=14)
                    [rand_item.show(ax=ax) if i == 0 else (Image(Tensor2ImgTensor(cb_fn(learn).on_batch_begin(xb, yb, True)['last_input'][rand_int]))
                     .apply_tfms(tfms).show(ax=ax)) for i, ax in enumerate(fig)]
                    #print(get_fn(cb).__name__, cb.keywords)
                    fig[0].set_title('original')
                    plt.show()
                    cb_tfms += 1
                    break
                except:
                    plt.close('all')

    elif ndim == 3:
        rand_item = TSItem(xb[rand_int])
        cb_tfms = 0
        for cb in learn.callback_fns:
            if get_fn(cb).__name__ == 'Recorder': continue
            if hasattr(cb, 'keywords') and hasattr(get_fn(cb), 'on_batch_begin'):
                cb_fn = partial(get_fn(cb), **cb.keywords)
                try:
                    fig = plt.subplots(rows, cols, figsize=figsize, sharex=True, sharey=True)[1].flatten()
                    plt.suptitle(get_fn(cb).__name__ , size=14)
                    [rand_item.show(ax=ax) if i == 0 else (TSItem(cb_fn(learn).on_batch_begin(xb, yb, True)['last_input'][rand_int])
                     .apply_tfms(tfms).show(ax=ax)) for i, ax in enumerate(fig)]
                    fig[0].set_title('original')
                    plt.show()
                    cb_tfms += 1
                    break
                except:
                    plt.close('all')


    if tfms is not None:
        fig = plt.subplots(rows, cols, figsize=figsize, sharex=True, sharey=True)[1].flatten()
        [rand_item.show(ax=ax) if i == 0 else rand_item.apply_tfms(tfms).show(ax=ax) for i, ax in enumerate(fig)]
        fig[0].set_title('original')
        try:
            t_ = []
            for t in learn.data.train_ds.tfms: t_.append(get_fn(t).__name__)
            title = f"{str(t_)[1:-1]} transforms applied"
            plt.suptitle(title, size=14)
        except: pass
        plt.show()
    elif cb_tfms == 0:
        print('No transformation has been applied')
        rand_item.show()

    return learn

Learner.show_tfms = show_tfms

def show_tfms_db(data, rows=3, cols=3, figsize=(8, 8)):
    xb, yb = data.one_batch()
    xb = xb.to('cpu')
    yb = yb.to('cpu')
    rand_int = np.random.randint(len(xb))
    ndim = xb.ndim
    tfms = data.train_ds.tfms

    if ndim == 4: rand_item = Image(xb[rand_int])
    elif ndim == 3:rand_item = TSItem(xb[rand_int])
    if tfms is not None:
        fig = plt.subplots(rows, cols, figsize=figsize, sharex=True, sharey=True)[1].flatten()
        [rand_item.show(ax=ax) if i == 0 else rand_item.apply_tfms(tfms).show(ax=ax) for i, ax in enumerate(fig)]
        fig[0].set_title('original')
        try:
            t_ = []
            for t in learn.data.train_ds.tfms: t_.append(get_fn(t).__name__)
            title = f"{str(t_)[1:-1]} transforms applied"
            plt.suptitle(title, size=14)
        except: pass
        plt.show()
    else:
        print('No transformation has been applied')
        rand_item.show()
    return

DataBunch.show_tfms = show_tfms_db

from torch.utils.data.sampler import WeightedRandomSampler

class OverSamplingCallback(LearnerCallback):
    def __init__(self,learn:Learner,weights:torch.Tensor=None):
        super().__init__(learn)
        self.weights = weights

    def on_train_begin(self, **kwargs):
        ds,dl = self.data.train_ds,self.data.train_dl
        self.labels = ds.y.items
        assert np.issubdtype(self.labels.dtype, np.integer), "Can only oversample integer values"
        _,self.label_counts = np.unique(self.labels,return_counts=True)
        if self.weights is None: self.weights = torch.DoubleTensor((1/self.label_counts)[self.labels])
        self.total_len_oversample = int(self.data.c*np.max(self.label_counts))
        sampler = WeightedRandomSampler(self.weights, self.total_len_oversample)
        self.data.train_dl = dl.new(shuffle=False, sampler=sampler)

def oversampling(learn: Learner) -> Learner:
    learn.callback_fns.append(OverSamplingCallback)
    return learn


Learner.oversampling = oversampling
Learner.os = oversampling


from fastai.callbacks.tracker import TrackerCallback
class ReduceLROnPlateau(TrackerCallback):
    "A `TrackerCallback` that reduces learning rate when a metric has stopped improving."
    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto',
                 patience:int=0, factor:float=0.2, min_delta:int=0, min_lr:float=1e-6, verbose=False):
        super().__init__(learn, monitor=monitor, mode=mode)
        self.patience,self.factor,self.min_delta,self.min_lr,self.verbose = patience,factor,min_delta,min_lr,verbose
        if self.operator == np.less:  self.min_delta *= -1

    def on_train_begin(self, **kwargs:Any)->None:
        "Initialize inner arguments."
        self.wait, self.opt = 0, self.learn.opt
        super().on_train_begin(**kwargs)

    def on_epoch_end(self, epoch, **kwargs:Any)->None:
        "Compare the value monitored to its best and maybe reduce lr."
        current = self.get_monitor_value()
        if current is None: return
        if self.operator(current - self.min_delta, self.best): self.best,self.wait = current,0
        elif self.opt.lr > self.min_lr:
            self.wait += 1
            if self.wait > self.patience:
                self.opt.lr = max(self.min_lr, self.opt.lr * self.factor)
                self.wait = 0
                if self.verbose: print(f'Epoch {epoch}: reducing lr to {self.opt.lr}')


def reduce_lr_on_plateau(learn: Learner, monitor='valid_loss', mode='auto',
                         patience=0, factor=0.2, min_delta=0, min_lr:float=1e-6, verbose=False) -> Learner:

    learn.callback_fns.append(
        partial(ReduceLROnPlateau, monitor=monitor, mode=mode,
                patience=patience, factor=factor, min_delta=min_delta, min_lr=min_lr, verbose=verbose))
    return learn

Learner.plateau = reduce_lr_on_plateau


import math

class TfmScheduler(LearnerCallback):

    def __init__(self,
                 learn: Learner,
                 tfm_fn: Callable,
                 sch_param: Union[str, StrList],
                 sch_val: Union[StartOptEnd, List],
                 sch_iter: Optional[StartOptEnd] = None,
                 sch_func: Optional[AnnealFunc] = None,
                 plot: bool = False,
                 test: bool = False,
                 **kwargs: Any):

        super().__init__(learn)
        self.learn = learn
        self.batches = math.ceil(len(learn.data.train_ds)/learn.data.train_dl.batch_size)
        sch_param = listify(sch_param)
        if isinstance(sch_param, (float, int)): sch_val = (0, sch_val)
        sch_val = tuplify(sch_val)
        if len(sch_param) != len(sch_val): sch_val = sch_val * len(sch_param)
        assert len(sch_param) == len(sch_val)
        if sch_iter is None: sch_iter = (0., 1.)
        sch_iter = tuplify(sch_iter)
        if len(sch_param) != len(sch_iter): sch_iter = sch_iter * len(sch_param)
        assert len(sch_param) == len(sch_iter)
        self.tfm_fn,self.sch_param,self.sch_val,self.sch_iter,self.test = tfm_fn,sch_param,sch_val,sch_iter,test
        if sch_func is None: sch_func = annealing_linear
        sch_func = listify(sch_func)
        if len(sch_param) != len(sch_func): sch_func = sch_func * len(sch_param)
        assert len(sch_param) == len(sch_func)
        self.sch_func = sch_func
        self.plot = plot
        if not isinstance(self.tfm_fn, functools.partial): self.tfm_fn = partial(self.tfm_fn)
        self.fn = get_fn(self.tfm_fn)
        if hasattr(self.fn, 'cb_fn'): self.fn = self.fn.cb_fn
        if hasattr(self.fn, 'on_batch_begin'): self.cb = True
        else: self.cb = False


    def on_train_begin(self, n_epochs: int, epoch: int, **kwargs: Any):
        if self.cb: self.fn(self.learn).on_train_begin()
        total_iters = n_epochs * self.batches
        self.scheduler = [None] * len(self.sch_param)
        for i in range(len(self.sch_param)):
            p = self.sch_param[i]
            v = self.sch_val[i]
            iters = self.sch_iter[i]
            func = self.sch_func[i]
            self.scheduler[i] = MyScheduler(total_iters, v, sch_iter=iters, sch_func=func)
            s = self.scheduler[i]
            a = s.start_val
            a_ = []
            first_iter = -1
            last_iter = 1
            for i in range(total_iters):
                a = s.step()
                if i > 0 and first_iter == -1 and a != a_[-1]: first_iter = (i - 1) / total_iters
                elif first_iter != -1 and last_iter == 1 and a == a_[-1]: last_iter = i / total_iters
                a_.append(a)
            s.restart()
            text = '{} between {} and {} in iters {:.2f} to {:.2f}'.format(
                p, round(min(a_), 5), round(max(a_), 5), first_iter, last_iter)
            print('\n',text)
            if self.plot:
                plt.plot(a_)
                plt.title(text)
                plt.show()

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        if self.test: return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}
        if train:
            for i, (p, v) in enumerate(zip(self.sch_param, self.sch_val)):
                new_v = self.scheduler[i].step()
                self.tfm_fn.keywords[p] = new_v
            kw = self.tfm_fn.keywords
            if self.cb:
                return self.fn(self.learn, **kw).on_batch_begin(
                    last_input=last_input,last_target=last_target,train=train)
            else:
                new_input = self.fn(last_input, **kw)
                return {'last_input': new_input, 'last_target': last_target}
        else: return

    def on_train_end(self, **kwargs):
        if self.cb: self.fn(self.learn).on_train_end()


class MyScheduler():
    "Used to \"step\" from start,end (`vals`) over `n_iter` iterations on a schedule defined by `func`"
    def __init__(self, total_iters:int, sch_val:StartOptEnd, sch_iter:Optional[StartOptEnd]=None,
                 sch_func:Optional[AnnealFunc]=None):
        self.total_iters = total_iters
        self.start_val,self.end_val = (sch_val[0],sch_val[1]) if is_tuple(sch_val) else (0, sch_val)
        if sch_iter is None: self.start_iter,self.end_iter = (0, total_iters)
        else:
            self.start_iter,self.end_iter = (sch_iter[0],sch_iter[1]) if is_tuple(sch_iter) else (0, sch_iter)
            if self.start_iter == 1 or isinstance(self.start_iter, float):
                self.start_iter = int(self.start_iter * total_iters)
            if self.end_iter == 1 or isinstance(self.end_iter, float):
                self.end_iter = int(self.end_iter * total_iters)
        self.eff_iters = self.end_iter - self.start_iter
        if sch_func is None: self.sch_func = annealing_linear
        else: self.sch_func = sch_func
        self.n = 0

    def restart(self): self.n = 0

    def step(self)->Number:
        "Return next value along annealed schedule."
        self.eff_n = min(max(0, self.n - self.start_iter), self.eff_iters)
        out = self.sch_func(self.start_val, self.end_val, min(1, self.eff_n/(self.eff_iters - 1)))
        self.n += 1
        return out


def cosine_annealing(start:Number, end:Number, pct:float, pct_start=.3, **kwargs)->Number:
    "Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0."
    if pct <= pct_start:
        return annealing_cos(start, end, pct/pct_start)
    else:
        return annealing_cos(end, start, (pct - pct_start)/(1 - pct_start))

def inv_annealing_poly(start:Number, end:Number, pct:float, degree:Number, **kwargs)->Number:
    "Helper function for `inv_anneal_poly`."
    return start + (end - start) * (pct)**degree

def inv_annealing_cos(start:Number, end:Number, pct:float, **kwargs)->Number:
    "Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0."
    cos_out = np.cos(np.pi * pct) + 1
    return start + (end - start)/2 * cos_out

def tuplify(a):
    if not isinstance(a, list): a = [a]
    for i, x in enumerate(a):
        if not isinstance(x, tuple): a[i] = (0, x)
    return a

def get_fn(a):
    while True:
        if hasattr(a, 'func'): a = a.func
        else: break
    return a