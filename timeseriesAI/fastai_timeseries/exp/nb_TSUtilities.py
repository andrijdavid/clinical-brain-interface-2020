#AUTOGENERATED! DO NOT EDIT! file to edit: ./TSUtilities.ipynb (unless otherwise specified)

import fastai
from fastai.basics import *
from fastai.vision import *
from fastai.tabular import *


import numpy as np
import pandas as pd
import scipy as sp
import matplotlib.pyplot as plt
import torch
import torchvision
import os
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")
import datetime
import pprint
from scipy.stats import ttest_ind
import sklearn
from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score
path = Path(os.getcwd())
from numbers import Integral
from IPython.display import display, HTML, clear_output
display(HTML("<style>.container { width:100% !important; }</style>"))

ts2img_kwargs = {}



def get_dpi():
    plt.plot([0, 1])
    plt.close()
    DPI = plt.gcf().get_dpi()
    plt.close()
    return int(DPI)


DPI = get_dpi()
defaults.dpi = DPI
device = defaults.device
cpus = defaults.cpus


def get_elements(arr, idx):
    from operator import itemgetter
    return itemgetter(*idx)(arr)

def cloning(list1):
    li_copy = list1[:]
    return li_copy



def scale(arr,
          train_stats=None,
          scaling_type='normalization',
          scaling_subtype='per_channel',
          scale_range=(-1, 1),
          **kwargs):

    if scaling_type is None: return arr, None
    if arr is None: return None, None
    if arr.ndim != 3:
        arr = To3dArray(arr)
    if train_stats is None:
        # Get train stats
        train = arr
        if scaling_type == 'normalization':
            if train.ndim == 2:
                if scaling_subtype == 'all_samples' or scaling_subtype == 'per_sample':
                    train_min = train.min(keepdims=True)
                    train_max = train.max(keepdims=True)
                elif scaling_subtype == 'per_channel':
                    train_min = train.min(axis=(1), keepdims=True)
                    train_max = train.max(axis=(1), keepdims=True)
                else:
                    print('***** Please, select a valid  scaling_subtype *****')
                    return
            elif train.ndim == 3:
                if scaling_subtype == 'all_samples':
                    train_min = train.min(keepdims=True)
                    train_max = train.max(keepdims=True)
                elif scaling_subtype == 'per_sample':
                    train_min = train.min(axis=(1, 2), keepdims=True)
                    train_max = train.max(axis=(1, 2), keepdims=True)
                elif scaling_subtype == 'per_channel':
                    train_min = train.min(axis=(0, 2), keepdims=True)
                    train_max = train.max(axis=(0, 2), keepdims=True)
                else:
                    print('***** Please, select a valid  scaling_subtype *****')
                    return
            train_stats = (train_min, train_max)
        elif scaling_type == 'standardization':
            if train.ndim == 2:
                if scaling_subtype == 'all_samples' or scaling_subtype == 'per_sample':
                    train_mean = train.mean(keepdims=True)
                    train_std = train.std(keepdims=True)
                elif scaling_subtype == 'per_channel':
                    train_mean = train.mean(axis=(1), keepdims=True)
                    train_std = train.std(axis=(1), keepdims=True)
                else:
                    print('***** Please, select a valid  scaling_subtype *****')
                    return
            elif train.ndim == 3:
                if scaling_subtype == 'all_samples':
                    train_mean = train.mean(keepdims=True)
                    train_std = train.std(keepdims=True)
                elif scaling_subtype == 'per_sample':
                    train_mean = train.mean(axis=(1, 2), keepdims=True)
                    train_std = train.std(axis=(1, 2), keepdims=True)
                elif scaling_subtype == 'per_channel':
                    train_mean = train.mean(axis=(0, 2), keepdims=True)
                    train_std = train.std(axis=(0, 2), keepdims=True)
                else:
                    print('***** Please, select a valid  scaling_subtype *****')
                    return
                train_stats = (train_mean, train_std)
        else:
            print('***** Please, select a valid  scaling_type *****')
            return

    # Calculate
    if scaling_type == 'normalization':
        train_min, train_max = train_stats
        if scaling_subtype == 'per_sample':
            return ((((arr - train_min)) / (train_max - train_min)) * (
                scale_range[1] - scale_range[0]) + scale_range[0], None)
        else:
            return ((((arr - train_min)) / (train_max - train_min)) * (
                scale_range[1] - scale_range[0]) + scale_range[0], (train_min, train_max))
            #return (To3dTensor((((arr - train_min)) / (train_max - train_min)) * (scale_range[1] - scale_range[0]) + scale_range[0]), (train_min, train_max))
    elif scaling_type == 'standardization':
        mean, std = train_stats
        if scaling_subtype == 'per_sample':
            return (np.nan_to_num((arr - mean) / std), None)
        else:
            return (np.nan_to_num((arr - mean) / std), (mean, std))
            #return (To3dTensor(np.nan_to_num((arr - mean) / std)), (mean, std))
    else:
        return

def scale_data(X_train,
           X_valid,
           X_test=None,
           scaling_type='normalization',
           scaling_subtype='per_channel',
           scale_range=(-1, 1)):

    X_train_sc, train_stats = scale(
        X_train,
        train_stats=None,
        scaling_type=scaling_type,
        scaling_subtype=scaling_subtype,
        scale_range=scale_range)

    X_valid_sc, _ = scale(
        X_valid,
        train_stats=train_stats,
        scaling_type=scaling_type,
        scaling_subtype=scaling_subtype,
        scale_range=scale_range)

    X_test_sc, _ = scale(
        X_test,
        train_stats=train_stats,
        scaling_type=scaling_type,
        scaling_subtype=scaling_subtype,
        scale_range=scale_range)

    if scaling_type == 'normalization':
        X_valid_sc = np.clip(X_valid_sc, scale_range[0], scale_range[1])
        if X_test is not None:
            X_test_sc = np.clip(X_test_sc, scale_range[0], scale_range[1])

    return X_train_sc, X_valid_sc, X_test_sc



def cap_outliers(y, lower=None, verbose=False):
    q25, q75 = np.percentile(y, 25), np.percentile(y, 75)
    iqr = q75 - q25
    cut_off = iqr * 1.5
    if lower is None:
        lower = q25 - cut_off
    upper = q75 + cut_off
    outliers = sorted([x for x in y if x < lower or x > upper])
    if verbose: print('outliers capped:', outliers, (lower, upper))
    y = np.array([min(max(x, lower), upper) for x in y])
    return y


def get_y_range(y, problem_type):
    if problem_type == 'regression':
        y_range = (int((y.min() - .5) * 2) / 2, math.ceil(
            (y.max() + .5) * 2) / 2)
        print('y_range:', y_range)
    else:
        y_range = None
    return y_range



def get_stratified_train_val_test_idxs(y,
                                       n_folds,
                                       test_fold=False,
                                       y_add=None,
                                       add_train_folds=None,
                                       oversample=False,
                                       seed=1):
    from sklearn.model_selection import StratifiedKFold
    if isinstance(y, np.ndarray): y = torch.Tensor(y).to(dtype=torch.int64)
    if n_folds == 1:
        folds = 5
    else:
        folds = n_folds
    if y_add:
        if add_train_folds:
            # Augmented dataset
            train_add_idx = list(
                zip(*list(
                    KFold(
                        n_splits=n_folds, shuffle=True, random_state=cv_seed).
                    split(np.zeros(len(y_add)), Y_train_add))))[0]
        else:
            train_add_idx = np.arange(len(y_add))
    else:
        train_add_idx = None
    if test_fold:
        outer_folds = list(
            StratifiedKFold(
                n_splits=folds + 1, shuffle=True, random_state=seed).split(
                    np.zeros(len(y)), y))
        test_idx = outer_folds[0][1]
        inner_idxs = outer_folds[0][0]
        inner_folds = StratifiedKFold(
            n_splits=folds, shuffle=True, random_state=seed).split(
                np.zeros(len(inner_idxs)), y[inner_idxs])
        train_idx = []
        val_idx = []
        for train, val in inner_folds:
            if oversample:
                train = oversampled_idxs(y[inner_idxs], train, seed=seed)
            train_idx.append(inner_idxs[train])
            val_idx.append(inner_idxs[val])
        if n_folds == 1:
            return [train_idx[0]], [val_idx[0]], test_idx
        return train_idx, val_idx, test_idx, train_add_idx
    else:
        inner_folds = StratifiedKFold(
            n_splits=folds, shuffle=True, random_state=seed).split(
                np.zeros(len(y)), y)
        train_idx = []
        val_idx = []
        for train, val in inner_folds:
            if oversample:
                train = oversampled_idxs(y, train, seed=seed)
            train_idx.append(train)
            val_idx.append(val)
        if n_folds == 1:
            return [train_idx[0]], [val_idx[0]], None
        return train_idx, val_idx, None, train_add_idx


def check_overlap(a, b):
    overlap = [i for i in a if i in b]
    if overlap == []:
        return
    return overlap


def leakage_finder(train, val, test=None):
    if check_overlap(train, val) is not None:
        print('train-val leakage!')
        return check_overlap(train, val)
    if test is not None:
        if check_overlap(train, test) is not None:
            print('train-test leakage!')
            return check_overlap(train, test)
        if check_overlap(val, test) is not None:
            print('val-test leakage!')
            return check_overlap(val, test)
    return


def oversampled_idxs(y, idx, seed=1, verbose=False):
    from imblearn.over_sampling import RandomOverSampler
    from collections import Counter
    ros = RandomOverSampler(random_state=seed)
    resampled_idxs, y_resampled = ros.fit_resample(idx.reshape(-1, 1), y[idx])
    if verbose:
        print('classes:', count_classes(y_resampled))
    return np.sort(resampled_idxs.ravel())


def split_data(X, y, train_idx, valid_idx, test_idx, train_add_idx=None):
    X_train, y_train = X[train_idx[0]], y[train_idx[0]]
    X_valid, y_valid = X[valid_idx[0]], y[valid_idx[0]]
    X_test, y_test = X[test_idx], y[test_idx]
    print('X_train:', X_train.shape, 'X_valid:', X_valid.shape, 'X_test:',
          X_test.shape)
    print('y_train:', y_train.shape, 'y_valid:', y_valid.shape, 'y_test:',
          y_test.shape)
    if train_add_idx is not None:
        X_train_add, y_train_add = X[train_add_idx[0]], y[train_add_idx[0]]
        print('X_train_add:', X_train_add.shape, 'y_train_add:',
              y_train_add.shape)
    else:
        X_train_add, y_train_add = None, None
    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_train_add, y_train_add


def count_classes(y):
    return dict(sorted(Counter(y).items()))


def get_class_weights(target):
    if isinstance(target, np.ndarray): target = torch.Tensor(target).to(dtype=torch.int64)
    # Compute samples weight (each sample should get its own weight)
    class_sample_count = torch.tensor(
        [(target == t).sum() for t in torch.unique(target, sorted=True)])
    weights = 1. / class_sample_count.float()
    return (weights / weights.sum()).to(device)

def get_weighted_sampler(target):
    from torch.utils.data.sampler import WeightedRandomSampler
    weight = get_class_weights(target)
    samples_weight = torch.tensor([weight[t] for t in target])
    # Create sampler, dataset, loader
    return WeightedRandomSampler(samples_weight, len(samples_weight))

def history_output(learn, max_lr, epochs, t0, t1):

    print('\ndataset                 :', learn.data.dsid)
    #print('shape                   :', learn.data.train_ds[0][0].data.shape[-2:])
    print('model                   :', learn.model.__name__)
    print('epochs                  :', epochs)
    print('batch size              :', learn.data.train_dl.batch_size)
    print('max_lr                  :', max_lr)
    print('wd                      :', learn.wd)
    print('time                    :',
          str(datetime.timedelta(seconds=(t1 - t0).seconds)))
    metrics = np.array(learn.recorder.metrics)
    metrics_names = learn.recorder.metrics_names
    train_loss = learn.recorder.losses
    best_train_epoch = np.nanargmin(train_loss)
    val_loss = learn.recorder.val_losses
    best_val_epoch = np.nanargmin(val_loss)
    epochs10p = min(5, max(1, int(.1 * epochs)))
    n_batches = len(learn.data.train_dl)
    b_per_eopch = math.ceil(len(learn.data.train_ds) / learn.data.train_dl.batch_size)


    print('\ntrain loss:')
    print('min train loss          : {:.5f}     epoch: {:}'.format(
        np.min(train_loss), (best_train_epoch + 1) // n_batches))
    print('final loss              : {:.5f}     epoch: {:}'.format(
        train_loss[-1], len(train_loss) // n_batches))
    print('final avg loss          : {:.5f} +/- {:.5f} in last {:} epochs'.format(
        np.mean(train_loss[-epochs10p*n_batches:]), np.std(train_loss[-epochs10p*n_batches:]), epochs10p))
    print('\nval loss:')
    print('min val loss            : {:.5f}     epoch: {:}'.format(
        np.min(val_loss), best_val_epoch + 1))
    print('final loss              : {:.5f}     epoch: {:}'.format(
        val_loss[-1], len(val_loss)))
    print('final avg loss          : {:.5f} +/- {:.5f} in last {:} epochs'.format(
        np.mean(val_loss[-epochs10p:]), np.std(val_loss[-epochs10p:]), epochs10p))
    if len(metrics) > 0:
        for i in range(0, metrics.shape[1]):
            metric = metrics[:, i]
            print()
            if not np.isnan(np.nanmax(metric)):
                print(metrics_names[i])
                print('highest metric          : {:.5f}     epoch: {:}'.format(
                    np.nanmax(metric),
                    np.nanargmax(metric) + 1))
                print('early stopping metric   : {:.5f}     epoch: {:}'.format(
                    metric[best_train_epoch // b_per_eopch], best_train_epoch // b_per_eopch + 1))
                print('final metric            : {:.5f}     epoch: {:}'.format(
                    metric[-1], len(metric)))
                print('final avg metric        : {:.5f} +/- {:.5f} in last {:} epochs'.format(
                    np.mean(metric[-epochs10p:]), np.std(metric[-epochs10p:]), epochs10p))

    learn.recorder.plot_lr()
    learn.recorder.plot_losses()
    if len(metrics) > 0: learn.recorder.plot_metrics()
    #try:
    #    interp = ClassificationInterpretation.from_learner(learn)
    #    interp.plot_confusion_matrix(figsize=(6,6), cmap='Greens')
    #except: pass
    return


def model_summary(model, data, find_all=False, print_mod=False):
    xb, yb = get_batch(data.valid_dl, model)
    mods = find_modules(model, is_lin_layer) if find_all else model.children()
    f = lambda hook,mod,inp,out: print(f"====\n{mod}\n" if print_mod else "", out.shape)
    with Hooks(mods, f) as hooks:
        learn.model(xb)


def get_batch(dl, learn):
    learn.xb, learn.yb = next(iter(dl))
    learn.do_begin_fit(0)
    learn('begin_batch')
    learn('after_fit')
    return learn.xb, learn.yb

def conv(ni, nf, ks=3, stride=1, bias=False):
    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)

def noopr(x, **kwargs):
    return x



def ToTensor(arr, **kwargs):
    if isinstance(arr, np.ndarray):
        arr = torch.from_numpy(arr)
    elif not isinstance(arr, torch.Tensor):
        print(f"Can't convert {type(arr)} to torch.Tensor")
    return arr.float()


def ToArray(arr):
    if isinstance(arr, torch.Tensor):
        arr = np.array(arr)
    elif not isinstance(arr, np.ndarray):
        print(f"Can't convert {type(arr)} to np.array")
    if arr.dtype == 'O': arr = np.array(arr, dtype=np.float32)
    return arr


def To3dTensor(arr):
    if arr.dtype == 'O': arr = np.array(arr, dtype=np.float32)
    arr = ToTensor(arr)
    if arr.ndim == 1: arr = arr[None, None]
    elif arr.ndim == 2: arr = arr[:, None]
    elif arr.ndim == 4: arr = arr[0]
    assert arr.ndim == 3, 'Please, review input dimensions'
    return arr


def To2dTensor(arr):
    if arr.dtype == 'O': arr = np.array(arr, dtype=np.float32)
    arr = ToTensor(arr)
    if arr.ndim == 1: arr = arr[None]
    elif arr.ndim == 3: arr = torch.squeeze(arr, 0)
    assert arr.ndim == 2, 'Please, review input dimensions'
    return arr


def To1dTensor(arr):
    if arr.dtype == 'O': arr = np.array(arr, dtype=np.float32)
    arr = ToTensor(arr)
    if arr.ndim == 3: arr = torch.squeeze(arr, 1)
    if arr.ndim == 2: arr = torch.squeeze(arr, 0)
    assert arr.ndim == 1, 'Please, review input dimensions'
    return arr


def To3dArray(arr):
    arr = ToArray(arr)
    if arr.ndim == 1: arr = arr[None, None]
    elif arr.ndim == 2: arr = arr[:, None]
    elif arr.ndim == 4: arr = arr[0]
    assert arr.ndim == 3, 'Please, review input dimensions'
    return np.array(arr)


def To2dArray(arr):
    arr = ToArray(arr)
    if arr.ndim == 1: arr = arr[None]
    if arr.ndim == 3: arr = np.squeeze(arr, 0)
    assert arr.ndim == 2, 'Please, review input dimensions'
    return np.array(arr)


def To1dArray(arr):
    arr = ToArray(arr)
    if arr.ndim == 3: arr = np.squeeze(arr, 1)
    if arr.ndim == 2: arr = np.squeeze(arr, 0)
    assert arr.ndim == 1, 'Please, review input dimensions'
    return np.array(arr)


def ToDevice(ts, **kwargs):
    if isinstance(ts, torch.Tensor):
        return ts.type(torch.FloatTensor)
    return torch.from_numpy(ts).type(torch.FloatTensor).to(device)
    if defaults.device.type == 'cpu':
        ts = torch.from_numpy(ts).double()  #.type(torch.FloatTensor)
        return ts
    else:
        ts = torch.from_numpy(ts).double().to(
            device)  #.type(torch.FloatTensor)
        return ts


cudify = partial(ToDevice)



def mape(pred, targ):
    "Mean absolute error between `pred` and `targ`."
    pred, targ = flatten_check(pred, targ)
    return (torch.abs(targ - pred) / targ).mean()


from fastai.metrics import RegMetrics
class MAPE(RegMetrics):
    "Compute the root mean absolute percentage error."
    def on_epoch_end(self, last_metrics, **kwargs):
        return add_metrics(last_metrics, mape(self.preds, self.targs))


from fastai.metrics import CMScores

class BPR(CMScores):
    'Balanced Precision Recall'
    _order = -20

    def __init__(self, alpha=1, beta=1):
        super().__init__()
        self.alpha, self.beta = alpha, beta

    def on_epoch_end(self, last_metrics, **kwargs):
        bpr = (min(self._precision(), self._recall())  ** self.alpha) * (max(self._precision(), self._recall()) ** self.beta)
        return add_metrics(last_metrics, bpr ** (1 / (self.alpha + self.beta)))


class FocalLoss(nn.Module):
    def __init__(self, α=.25, γ=2., weight=None):
        super().__init__()
        self.α = α
        self.γ = γ
        self.weight = weight

    def forward(self, inputs, targets, **kwargs):
        CE_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='sum')(inputs, targets)
        pt = torch.exp(-CE_loss)
        F_loss = self.α * ((1 - pt)**self.γ) * CE_loss
        return F_loss.mean()


def get_model_hp(tsmodel, kwargs=[{}]):
    all_args = inspect.getargspec(tsmodel.__dict__['__init__'])
    if all_args.defaults:
        tsmodel_dict = dict(zip(all_args.args[-len(all_args.defaults):], all_args.defaults))
        if kwargs != [{}]:
            for i in range(len(listify(kwargs))):
                for k,v in kwargs[i].items(): tsmodel_dict[k] = v
        return tsmodel_dict
    else: return {}


def get_outcome_stats (learn, y_outcome, problem_type, train, valid, test=None, thr=None):
    train_pred, train_true = learn.get_preds(DatasetType.Fix)
    train_true = train_true.numpy()  #.astype(int)
    if problem_type == 'regression':
        train_preds = train_pred.numpy().ravel()  #.astype(int)
    else:
        train_preds = train_pred[:, 1].numpy()

    if thr is not None: max_thr = thr
    elif problem_type == 'regression':
        max_thr = -1000
        max_sum = -1000
        sum_ = []
        xrange = np.arange(
                int(np.min(train_preds) // .001) * .001,
                int(1 + np.max(train_preds) // .001) * .001, .001)
        for i in xrange:
            thr = i
            y_true_train = y_outcome[train]
            train_preds_sum = y_true_train[train_preds >= thr].sum()
            sum_.append(train_preds_sum)
            if train_preds_sum > max_sum:
                max_thr = thr
                max_sum = train_preds_sum
    else: max_thr = .5
    y_true_train = y_outcome[train]
    pred_train_trades = y_true_train[train_preds >= max_thr]

    valid_pred, valid_true = learn.get_preds(DatasetType.Valid)
    if problem_type == 'regression':
        valid_preds = np.array(valid_pred).ravel()  #.astype(int)
    else:
        valid_preds = valid_pred[:, 1].numpy()
    y_true_valid = y_outcome[valid]
    pred_val_trades = y_true_valid[valid_preds >= max_thr]

    try:
        test_pred, test_true = learn.get_preds(DatasetType.Test)
    except:
        test_pred = []
    if problem_type == 'regression':
        test_preds = np.array(test_pred).ravel()  #.astype(int)
    else:
        test_preds = test_pred[:, 1].numpy()
    y_true_test = y_outcome[test]
    pred_test_trades = y_true_test[test_preds >= max_thr]

    if thr is None and problem_type == 'regression':
        plt.plot(xrange, sum_)
        plt.xticks(np.arange(int(np.min(xrange) // .05) * .05, np.max(xrange)+.2, .2))
        plt.show()


    print('Thr  : {:.2f}'.format(max_thr))
    print('TRAIN: %trades: {:3.1%}  avgR: {:+1.3f}({:+1.3f}) win%: {:2.1%}'.format(
        len(pred_train_trades) / len(y_true_train), pred_train_trades.mean(),
        y_true_train.mean(), (pred_train_trades > 0).mean()))

    print('VALID: %trades: {:3.1%}  avgR: {:+1.3f}({:+1.3f}) win%: {:2.1%}'.format(
        len(pred_val_trades) / len(y_true_valid), pred_val_trades.mean(),
        y_true_valid.mean(), (pred_val_trades > 0).mean()))

    if test_pred != []:
        print('TEST : %trades: {:3.1%}  avgR: {:+1.3f}({:+1.3f}) win%: {:2.1%}'.format(
            len(pred_test_trades) / len(y_true_test), pred_test_trades.mean(),
            y_true_test.mean(), (pred_test_trades > 0).mean()))


    return


def get_last_pos(arr, val, ndigits=5):
    if round(arr[-1], ndigits) != val: return 0
    for i,v in enumerate(arr[::-1]):
        if round(v, ndigits) != val: return i-1

def plot_weights(learn):
    w1 = np.sort(list(learn.model.parameters())[0].data.flatten())
    plt.hist(w1, 50, color='blue')
    plt.xlim([-1.75, 1.75])
    plt.title('Layer init weights mean: {:.5f} std: {:.5f}'.format(
        w1.mean(), w1.std()))
    plt.grid()
    plt.show()

def load_params(m, path):
    m.load_state_dict(torch.load(path))
    return m.to(device)


def noop(x): return x

def get_layers(model, cond=noop):
    if isinstance(model, Learner): model=model.model
    return [m for m in flatten_model(model) if any([c(m) for c in listify(cond)])]

def count_params(model):
    if isinstance(model, Learner): model = model.model
    count = 0
    for l in get_layers(model):
        for i in range(len(list(l.parameters()))):
            count += len(list(l.parameters())[i].data.flatten())
    return count


def nb_auto_export():
    from IPython.display import display, Javascript
    import time
    from notebook2script import notebook2script
    display(Javascript('IPython.notebook.save_notebook()'))
    time.sleep(1)
    notebook2script()
    display(Javascript('IPython.notebook.save_checkpoint()'))