#AUTOGENERATED! DO NOT EDIT! file to edit: ./Initialization.ipynb (unless otherwise specified)

# To develop .zmuv() I've taken ideas from:
# - https://github.com/ducha-aiki/LSUV-pytorch/blob/master/LSUV.py
# and from the fastai community, in particular from Jeremy, @kevinB and @SteveR

from fastai.torch_core import *
from fastai.basic_train import Learner

device = 'cuda' if torch.cuda.is_available() else 'cpu'


class ListContainer():
    def __init__(self, items):
        self.items = listify(items)
    def __getitem__(self, idx):
        try:
            print(idx)
            return self.items[idx]
        except TypeError:
            if isinstance(idx[0], bool):
                assert len(idx)==len(self)
                return [o for m,o in zip(idx,self.items) if m]
            return [self.items[i] for i in idx]
    def __len__(self): return len(self.items)
    def __iter__(self): return iter(self.items)
    def __setitem__(self, i, o): self.items[i] = o
    def __delitem__(self, i): del(self.items[i])
    def __repr__(self):
        res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
        if len(self)>10: res = res[:-1]+ '...]'
        return res

class Hook():
    def __init__(self, m, f):
        self.hook = m.register_forward_hook(partial(f, self))
    def remove(self): self.hook.remove()
    def __del__(self): self.remove()

class Hooks(ListContainer):
    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
    def __enter__(self, *args): return self
    def __exit__ (self, *args): self.remove()
    def __del__(self): self.remove()
    def __delitem__(self, i):
        self[i].remove()
        super().__delitem__(i)
    def remove(self):
        for h in self: h.remove()


def noop(x): return x

def is_layer(*args):
    def _is_layer(l, cond=args):
        return isinstance(l, cond)
    return partial(_is_layer, cond=args)

def is_lin_layer(l):
    return isinstance(l, nn.Linear)

def is_conv_lin_layer(l):
    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)
    return isinstance(l, lin_layers)

def is_affine_layer(l):
    return has_bias(l) or has_weight(l)

def is_conv_layer(l):
    conv_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d)
    return isinstance(l, conv_layers)

def has_bias(l):
    return (hasattr(l, 'bias') and l.bias is not None)

def has_weight(l):
    return (hasattr(l, 'weight'))

def has_weight_or_bias(l):
    return any((has_weight(l), has_bias(l)))

def find_modules(m, cond=noop):
    if isinstance(m, Learner): m=m.model
    if cond(m): return [m]
    return sum([find_modules(o,cond) for o in m.children()], [])

def get_layers(model, cond=noop):
    if isinstance(model, Learner): model=model.model
    return [m for m in flatten_model(model) if any([c(m) for c in listify(cond)])]

Learner.layers = get_layers



def append_stat(hook, mod, inp, out):
    i = inp[0].data
    try: o = out.data
    except: o = out[0].data # for RNNs
    hook.inp_mean, hook.inp_std = i.mean().item(), i.std().item()
    hook.out_mean, hook.out_std = o.mean().item(), o.std().item()


def zmuv_layer(model: Callable, m: Callable, xb: Tensor, max_attempts: int = 5,
                exp_mean: float = 0., exp_std: float = 1., tol: float = 1e-5,
               ε: float = 1e-8):
    h = Hook(m, append_stat)
    attempts = 0
    while model(xb) is not None:
        if attempts == 0: pre_mean, pre_std = h.out_mean, h.out_std
        if has_weight(m) and abs(h.out_std - exp_std) > tol: m.weight.data *= exp_std / (h.out_std + ε)
        elif has_bias(m) and abs(h.out_mean - exp_mean) > tol: m.bias.data -= h.out_mean - exp_mean
        else: break
        attempts += 1
        if attempts >= max_attempts: break
    h.remove()
    return pre_mean, pre_std, h.out_mean, h.out_std


def zmuv(learn: Learner,  tol: float = 1e-5, exp_mean: float = 0.,
         exp_std: float = 1., orthonorm: bool = True,
         cond: callable = has_weight_or_bias, verbose: bool = False) -> Learner:
    print('ZMUV initialization...')
    xb, yb = next(iter(learn.data.train_dl))
    if orthonorm: learn.model.apply(orthogonal_weights_init)
    mods = get_layers(learn.model, cond=cond)
    mean_act, std_act = [], []
    from fastprogress import progress_bar
    from time import sleep
    pb = progress_bar(mods)
    for m in pb:
        sleep(0.01)
        if has_weight(m) or has_bias(m):
            pre_mean, pre_std, mean, std = zmuv_layer(learn.model, m, xb, tol=tol,
                                                      exp_mean=exp_mean,exp_std=exp_std)
            if mean==0 and std==0: continue
            mean_act.append(mean)
            std_act.append(std)
            if verbose >= 2:
                print(m)
                print('     pre-zmuv activations    :  mean = {:9.5f}   std = {:9.5f}'.
                    format(pre_mean, pre_std))
                print('     post-zmuv activations   :  mean = {:9.5f}   std = {:9.5f}'.
                    format(mean, std))
    print('\noverall post-zmuv activations:  mean = {:9.5f}   std = {:9.5f}'
        .format(np.mean(mean_act), np.mean(std_act)))
    print('...ZMUV initialization complete\n')
    return learn

Learner.zmuv = zmuv

def svd_orthonormal(w):
    shape = w.shape
    if len(shape) < 2: flat_shape = (shape[0], 1)
    else: flat_shape = (shape[0], np.prod(shape[1:]))
    a = np.random.normal(0.0, 1.0, flat_shape)  #w;
    u, _, v = np.linalg.svd(a, full_matrices=False)
    q = u if u.shape == flat_shape else v
    q = q.reshape(shape)
    return q.astype(np.float32)


device = 'cuda' if torch.cuda.is_available() else 'cpu'
def orthogonal_weights_init(m):
    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)
    if isinstance(m, lin_layers):
        if hasattr(m, 'weight_v'):
            w_ortho = svd_orthonormal(m.weight_v.data.cpu().numpy())
            m.weight_v.data = torch.from_numpy(w_ortho).to(device)
            try:nn.init.constant_(m.bias, 0)
            except:pass
        else:
            w_ortho = svd_orthonormal(m.weight.data.cpu().numpy())
            m.weight.data = torch.from_numpy(w_ortho).to(device)
            try:nn.init.constant_(m.bias, 0)
            except:pass
    return


def ortho_w_init(learn: Learner) -> Learner:
    learn.model.apply(orthogonal_weights_init)
    return learn

Learner.ortho = ortho_w_init



def kaiming(learn: Learner, normal: bool = True, a: float = 0,
            verbose:bool=False) -> Learner:
    f = nn.init.kaiming_normal_ if normal else nn.init.kaiming_uniform_
    for i, m in enumerate(get_layers(learn, cond=noop)):
        if hasattr(m, 'weight') and m.weight.ndim >= 2:
            if verbose: print(i,'w',m)
            f(m.weight, a=a)
        if has_bias(m):
            if verbose: print(i,'b',m)
            m.bias.data.zero_()
    return learn

Learner.kaiming = kaiming


def activations(learn, thr=.1, cond=noop) -> Learner:
    mods = get_layers(learn, cond=cond)
    xb, yb = next(iter(learn.data.train_dl))
    with Hooks(mods, layer_stats) as hooks:
        learn.model(xb)
        ms, inp_act_mean, out_act_mean, inp_act_std, out_act_std, inp_dead, \
        out_dead, inp_size, out_size = [], [], [], [], [], [], [], [], []
        i = 0
        for m, h in zip(mods, hooks):
            if not hasattr(h, 'out_mean') or\
             (h.out_mean == h.inp_mean and h.out_std == h.inp_std): continue
            ms.append(' {:3d}-{} {}'.format(i,type(m).__name__,h.out_size[1:]))
            inp_act_mean.append(h.inp_mean)
            out_act_mean.append(h.out_mean)
            inp_act_std.append(h.inp_std)
            out_act_std.append(h.out_std)
            inp_dead.append(h.inp_dead)
            out_dead.append(h.out_dead)
            inp_size.append(h.inp_size)
            out_size.append(h.out_size)
            i += 1

    width = 0.4

    # MEAN
    x = inp_act_mean
    z = out_act_mean
    y = np.arange(len(x))
    figsize = (10, len(y) * .3)

    plt.figure(figsize=figsize)
    plt.barh(y - width / 2, x, width, color='lavender', align='center', label='input')
    plt.barh(y + width / 2,z,width,color='purple',align='center',label='output')
    plt.vlines(0, y[0] - 1, y[-1] + 1, linewidth=3, color='g', label='expected')
    plt.vlines(np.mean(z),y[0] - 1,y[-1] + 1,linewidth=3,color='r',label='actual')
    plt.yticks(y, ms)
    plt.xlim(min(-1., min(x) - .1, min(z) - .1), max(1, max(x) + .1, max(z) + .1))
    plt.ylim(len(y), -1)
    plt.legend(loc='best')
    plt.title('Initial activations (mean)')
    plt.grid(axis='x', color='silver', alpha=.3)
    plt.show()

    #STD
    x = inp_act_std
    z = out_act_std
    y = np.arange(len(x))
    plt.figure(figsize=figsize)
    plt.barh(
        y - width / 2, x, width, color='lavender', align='center', label='input')
    plt.barh(y + width / 2,z,width,color='purple',align='center',label='output')
    plt.vlines(1, y[0] - 1, y[-1] + 1, linewidth=3, color='g', label='expected')
    plt.vlines(np.mean(z),y[0] - 1,y[-1] + 1,linewidth=3,color='r',label='actual')
    plt.yticks(y, ms)
    plt.xlim(0, max(1.2, max(1, max(x) + .1, max(z) + .1)))
    plt.ylim(len(y), -1)
    plt.legend(loc='best')
    plt.title('Initial activations (std)')
    plt.grid(axis='x', color='silver', alpha=.3)
    plt.show()

    #DEAD
    x = inp_dead
    z = out_dead
    max_exp = .1
    plt.figure(figsize=figsize)
    plt.barh(y - width / 2, x, width, color='lavender', align='center', label='input')
    plt.barh( y + width / 2,z,width, color='purple', align='center',label='output')
    plt.vlines(max_exp, y[0] - 1, y[-1] + 1, linewidth=3, color='g', label='expected')
    plt.vlines(np.mean(z),y[0] - 1,y[-1] + 1,linewidth=3, color='r',label='actual')
    plt.yticks(y, ms)
    plt.xlim(0, 1)
    plt.ylim(len(y), -1)
    plt.legend(loc='best')
    plt.title('Initial dead activations <={}'.format(str(thr)))
    plt.grid(axis='x', color='silver', alpha=.3)
    plt.show()

    return learn

Learner.activations = activations
Learner.act = activations

def layer_stats(hook, mod, inp, out, thr=.1):
    i = inp[0].data
    o = out.data
    hook.inp_mean, hook.inp_std = i.mean().item(), i.std().item()
    hook.out_mean, hook.out_std = o.mean().item(), o.std().item()
    hook.inp_size = list(i.shape)
    hook.out_size = list(o.shape)
    hook.inp_dead = len(i[torch.abs(i)<=thr].flatten()) / len(i.flatten())
    hook.out_dead = len(o[torch.abs(o)<=thr].flatten()) / len(o.flatten())