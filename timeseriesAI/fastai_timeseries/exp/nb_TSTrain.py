#AUTOGENERATED! DO NOT EDIT! file to edit: ./TSTrain.ipynb (unless otherwise specified)

from fastai.callbacks import *

try:
    from exp.nb_TSUtilities import *
    from exp.nb_TSBasicData import *
    from exp.nb_TSDatasets import *

except ImportError:
    from .nb_TSUtilities import *
    from .nb_TSBasicData import *
    from .nb_TSDatasets import *

import time


def run_UCR_test(iters, epochs, datasets, arch,
                 bs=64, max_lr=3e-3, pct_start=.7, warmup=False, wd=1e-2,
                 metrics=[accuracy], mixup=False,
                 scale_type ='standardize', scale_subtype='per_channel', scale_range=(-1, 1),
                 opt_func=functools.partial(torch.optim.Adam, betas=(0.9, 0.99)),
                 loss_func=None, **arch_kwargs):
    ds_, acc_, acces_, accmax_, iter_, time_, epochs_, loss_, val_loss_   = [], [], [], [], [], [], [], [], []
    datasets = listify(datasets)
    for ds in datasets:
        db = create_UCR_databunch(ds)
        for i in range(iters):
            print('\n', ds, i)
            ds_.append(ds)
            iter_.append(i)
            epochs_.append(epochs)
            model = arch(db.features, db.c, **arch_kwargs).to(defaults.device)
            learn = Learner(db, model, opt_func=opt_func, loss_func=loss_func)
            if mixup: learn.mixup()
            learn.metrics = metrics
            start_time = time.time()
            learn.fit_one_cycle(epochs, max_lr=max_lr, pct_start=pct_start, moms=(.95, .85) if warmup else (.95, .95),
                                div_factor=25.0 if warmup else 1., wd=wd)
            duration = time.time() - start_time
            time_.append('{:.0f}'.format(duration))
            early_stop = math.ceil(np.argmin(learn.recorder.losses) / len(learn.data.train_dl))
            acc_.append(learn.recorder.metrics[-1][0].item())
            acces_.append(learn.recorder.metrics[early_stop - 1][0].item())
            accmax_.append(np.max(learn.recorder.metrics))
            loss_.append(learn.recorder.losses[-1].item())
            val_loss_.append(learn.recorder.val_losses[-1].item())
            if len(datasets) * iters >1: clear_output()
            df = (pd.DataFrame(np.stack((ds_, iter_, epochs_, loss_, val_loss_ ,acc_, acces_, accmax_, time_)).T,
                               columns=['dataset', 'iter', 'epochs', 'loss', 'val_loss',
                                        'accuracy', 'accuracy_ts',
                                        'max_accuracy', 'time (s)'])
                  )
            df = df.astype({'loss': float, 'val_loss': float, 'accuracy': float,
                            'accuracy_ts': float, 'max_accuracy': float})
            display(df)
    return learn, df



def FlatCosAnnealScheduler(learn, lr:float=4e-3, epochs:int=1, moms:Floats=(0.85,0.95),
                          start_pct:float=0.7, curve='cosine'):
    "Manage FCFit trainnig as found in the ImageNette experiments"
    #n = len(learn.data.train_dl)
    import math
    n = math.ceil(len(learn.data.train_ds)/learn.data.train_dl.batch_size)
    anneal_start = int(n * epochs * start_pct)
    batch_finish = ((n * epochs) - anneal_start)
    if curve=="cosine":        curve_type=annealing_cos
    elif curve=="linear":      curve_type=annealing_linear
    elif curve=="exponential": curve_type=annealing_exp
    else: raiseValueError(f"annealing type not supported {curve}")
    phase0 = TrainingPhase(anneal_start).schedule_hp('lr', lr).schedule_hp('mom', moms[0])
    phase1 = (TrainingPhase(batch_finish)
    .schedule_hp('lr', lr, anneal=curve_type)
    .schedule_hp('mom', (moms[0],moms[1]), anneal=curve_type)
    )
    phases = [phase0, phase1]
    return GeneralScheduler(learn, phases)

def fit_fc(learn:Learner, epochs:int, lr:float=4e-3,
           moms:Tuple[float,float]=(0.85,0.95), start_pct:float=0.7,
           wd:float=None, callbacks:Optional[CallbackList]=None,
           show_curve:bool=False)->None:
    "Fit a model with Flat Cosine Annealing"
    max_lr = learn.lr_range(lr)
    callbacks = listify(callbacks)
    callbacks.append(FlatCosAnnealScheduler(learn, lr, moms=moms, start_pct=start_pct, epochs=epochs))
    learn.fit(epochs, max_lr, wd=wd, callbacks=callbacks)
    if show_curve: learn.recorder.plot_lr()