{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T13:51:04.214020Z",
     "start_time": "2020-05-26T13:51:03.387323Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -e ./tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:38.458298Z",
     "start_time": "2020-05-26T14:40:37.876330Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import pickle \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "x, y = pickle.load(open(\"data/train.pkl\", \"rb\"))\n",
    "test_name, x_test = pickle.load(open(\"data/test.pkl\", \"rb\"))\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:38.767560Z",
     "start_time": "2020-05-26T14:40:38.759288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 12, 4096)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:39.933872Z",
     "start_time": "2020-05-26T14:40:39.212858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/pytorch/lib/python3.7/site-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.layers import *\n",
    "\n",
    "def convlayer(c_in,c_out,ks=3,padding='same',bias=True,stride=1,\n",
    "              bn_init=False,zero_bn=False,bn_before=True,\n",
    "              act_fn='relu', **kwargs):\n",
    "    '''conv layer (padding=\"same\") + bn + act'''\n",
    "    if ks % 2 == 1 and padding == 'same': padding = ks // 2\n",
    "    layers = [ConvSP1d(c_in,c_out, ks, bias=bias, stride=stride) if padding == 'same' else \\\n",
    "    nn.Conv1d(c_in,c_out, ks, stride=stride, padding=padding, bias=bias)]\n",
    "    bn = GBN(c_out)\n",
    "    if bn_init: nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    if bn_before: layers.append(bn)\n",
    "    if act_fn: layers.append(get_act_layer(act_fn, **kwargs))\n",
    "    if not bn_before: layers.append(bn)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=[7, 5, 3], act_fn='relu'):\n",
    "        super().__init__()\n",
    "        self.conv1 = convlayer(ni, nf, ks[0], act_fn=act_fn)\n",
    "        self.conv2 = convlayer(nf, nf, ks[1], act_fn=act_fn)\n",
    "        self.conv3 = convlayer(nf, nf, ks[2], act_fn=False)\n",
    "\n",
    "        # expand channels for the sum if necessary\n",
    "        self.shortcut = noop if ni == nf else convlayer(ni, nf, ks=1, act_fn=False)\n",
    "        self.act_fn = get_act_layer(act_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        sc = self.shortcut(res)\n",
    "        x += sc\n",
    "        x = self.act_fn(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,c_in, c_out, pool=nn.AdaptiveAvgPool1d, act_fn='relu'):\n",
    "        super().__init__()\n",
    "        nf = 64\n",
    "\n",
    "        self.block1 = ResBlock(c_in, nf, ks=[7, 5, 3], act_fn=act_fn)\n",
    "        self.block2 = ResBlock(nf, nf * 2, ks=[7, 5, 3], act_fn=act_fn)\n",
    "        self.block3 = ResBlock(nf * 2, nf * 2, ks=[7, 5, 3], act_fn=act_fn)\n",
    "        self.gap = pool(1)\n",
    "        self.fc = nn.Linear(nf * 2, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:40.372144Z",
     "start_time": "2020-05-26T14:40:40.027376Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "import numpy as np\n",
    "from tabnet.pytorch_tabnet import sparsemax\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNetNoEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Number of features\n",
    "        - output_dim : int\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        - n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        - n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        - n_steps: int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        - gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        - n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        - n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        - epsilon: float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        \"\"\"\n",
    "        super(TabNetNoEmbeddings, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(ConvSP1d(self.input_dim,\n",
    "                                                        2*(n_d + n_a),1,\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(ConvSP1d(n_d + n_a, 2*(n_d + n_a),1, bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum)\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = Flatten()\n",
    "        self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
    "        initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_explain = torch.zeros(x.shape).to(x.device)\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1)) / (self.n_steps)\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "        \n",
    "        res = self.gap(res)\n",
    "        res = self.flatten(res)\n",
    "        res = self.final_mapping(res)\n",
    "        return res\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, virtual_batch_size=128, momentum=0.02):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Input size\n",
    "        - output_dim : int\n",
    "            Outpu_size\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = ConvSP1d(input_dim, output_dim,1, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        # Sparsemax\n",
    "        self.sp_max = sparsemax.Sparsemax(dim=-1)\n",
    "        # Entmax\n",
    "        # self.sp_max = sparsemax.Entmax15(dim=-1)\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.sp_max(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Input size\n",
    "        - output_dim : int\n",
    "            Outpu_size\n",
    "        - n_glu_independant\n",
    "        - shared_blocks : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Feat Transform Input\", x.shape)\n",
    "        x = self.shared(x)\n",
    "        print(\"Feat Transform Shared\",x.shape)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = ConvSP1d(input_dim, 2*output_dim,1, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:40.750371Z",
     "start_time": "2020-05-26T14:40:40.478863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeseries import *\n",
    "from models import *\n",
    "import pickle \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastai.distributed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:43.535253Z",
     "start_time": "2020-05-26T14:40:41.391943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSDataBunch;\n",
       "\n",
       "Train: LabelList (512 items)\n",
       "x: TSList\n",
       "TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096)\n",
       "y: CategoryList\n",
       "1,2,2,2,1\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (128 items)\n",
       "x: TSList\n",
       "TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096),TimeSeries(ch=12, seq_len=4096)\n",
       "y: CategoryList\n",
       "1,2,2,2,2\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defaults.device = torch.device('cpu')\n",
    "# fastai.torch_core.defaults.device = torch.device('cpu')\n",
    "    \n",
    "scale_type = 'normalize'\n",
    "scale_by_channel = False\n",
    "scale_by_sample  = True \n",
    "scale_range = (-1, 1)\n",
    "bs=128\n",
    "data = (ItemLists(Path(\"data\"), TSList(x_train),TSList(x_val))\n",
    "        .label_from_lists(y_train, y_val)\n",
    "        .databunch(bs=bs, val_bs=bs * 2)\n",
    "        .scale(scale_type=scale_type, scale_by_channel=scale_by_channel, \n",
    "             scale_by_sample=scale_by_sample,scale_range=scale_range)\n",
    "     )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:43.694475Z",
     "start_time": "2020-05-26T14:40:43.692219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:45.452238Z",
     "start_time": "2020-05-26T14:40:43.872322Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet(data.features, data.c).to(defaults.device)\n",
    "kappa = KappaScore()\n",
    "learn = Learner(data, model, metrics=[accuracy, kappa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:40:45.617926Z",
     "start_time": "2020-05-26T14:40:45.614924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (block1): ResBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(12, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(12, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_fn): ReLU()\n",
       "  )\n",
       "  (block2): ResBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_fn): ReLU()\n",
       "  )\n",
       "  (block3): ResBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_fn): ReLU()\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-26T14:40:12.695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='98' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      98.00% [98/100 04:19<00:05]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kappa_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.734929</td>\n",
       "      <td>0.693660</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706190</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.695487</td>\n",
       "      <td>0.693309</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.684111</td>\n",
       "      <td>0.693241</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.674993</td>\n",
       "      <td>0.693467</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.016605</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.664864</td>\n",
       "      <td>0.693235</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.115280</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.655644</td>\n",
       "      <td>0.695939</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>0.050584</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.652517</td>\n",
       "      <td>0.682703</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.647595</td>\n",
       "      <td>0.704779</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641123</td>\n",
       "      <td>0.676482</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.243908</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.635580</td>\n",
       "      <td>0.641370</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.212405</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.633819</td>\n",
       "      <td>1.232299</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.630493</td>\n",
       "      <td>0.635332</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.211628</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.625226</td>\n",
       "      <td>0.761766</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.196329</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.621249</td>\n",
       "      <td>1.373866</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>-0.020536</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.612550</td>\n",
       "      <td>0.743477</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.253826</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.608432</td>\n",
       "      <td>0.657908</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.280020</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.607674</td>\n",
       "      <td>1.548486</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.227240</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.602383</td>\n",
       "      <td>2.715894</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.164120</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.608007</td>\n",
       "      <td>0.772051</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.314341</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.611378</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.610402</td>\n",
       "      <td>0.661847</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.235495</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.606976</td>\n",
       "      <td>2.420270</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.045465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>1.622152</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.600801</td>\n",
       "      <td>1.116908</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.433767</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.597358</td>\n",
       "      <td>4.109869</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.017579</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.592782</td>\n",
       "      <td>1.028498</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.227613</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.585778</td>\n",
       "      <td>0.586712</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.338094</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.577975</td>\n",
       "      <td>0.588379</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.276125</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.569582</td>\n",
       "      <td>0.548367</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.608611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.565359</td>\n",
       "      <td>0.528391</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.369613</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.558410</td>\n",
       "      <td>1.780142</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.150579</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.551558</td>\n",
       "      <td>1.644662</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.120540</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.547285</td>\n",
       "      <td>0.568916</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.484627</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.540935</td>\n",
       "      <td>1.090332</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.136177</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.535373</td>\n",
       "      <td>1.055264</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.053021</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.532088</td>\n",
       "      <td>0.938392</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.181304</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.527520</td>\n",
       "      <td>0.551783</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.483366</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.521069</td>\n",
       "      <td>1.413589</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.514851</td>\n",
       "      <td>2.235691</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.509012</td>\n",
       "      <td>0.796025</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.498408</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.502704</td>\n",
       "      <td>0.511313</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.529527</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>0.612190</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.626186</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.486621</td>\n",
       "      <td>0.586349</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.433489</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.478461</td>\n",
       "      <td>2.913775</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.049740</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.473247</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500122</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.469852</td>\n",
       "      <td>0.587630</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.450442</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.463584</td>\n",
       "      <td>0.473540</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.608611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.455440</td>\n",
       "      <td>0.551938</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.549077</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.445550</td>\n",
       "      <td>1.437965</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.105327</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.440458</td>\n",
       "      <td>2.636827</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>-0.003861</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.430358</td>\n",
       "      <td>2.459615</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.074922</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.421820</td>\n",
       "      <td>0.621705</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.512531</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>1.234776</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.455806</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.608611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.393645</td>\n",
       "      <td>0.751602</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.455253</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.382654</td>\n",
       "      <td>0.492387</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.371501</td>\n",
       "      <td>0.526867</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.626368</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.364092</td>\n",
       "      <td>0.726947</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.471716</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.358107</td>\n",
       "      <td>0.483261</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.548199</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.350912</td>\n",
       "      <td>0.555792</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.531136</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.343544</td>\n",
       "      <td>0.531445</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.417896</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.332971</td>\n",
       "      <td>0.563020</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.528140</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.322123</td>\n",
       "      <td>0.428267</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.609946</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.311649</td>\n",
       "      <td>0.456682</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.579562</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.300563</td>\n",
       "      <td>0.613876</td>\n",
       "      <td>0.679688</td>\n",
       "      <td>0.364956</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.289384</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.272375</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.280826</td>\n",
       "      <td>0.335570</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.781303</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.273566</td>\n",
       "      <td>0.399367</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.670750</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.266973</td>\n",
       "      <td>0.399190</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.641151</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.258242</td>\n",
       "      <td>0.386738</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.797073</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.251080</td>\n",
       "      <td>0.566013</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.455518</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.242080</td>\n",
       "      <td>0.507077</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.563672</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.234341</td>\n",
       "      <td>0.301709</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.765510</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.325022</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.734115</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.217789</td>\n",
       "      <td>0.471274</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.548419</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.209662</td>\n",
       "      <td>0.558748</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.565154</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.201427</td>\n",
       "      <td>0.548446</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.502550</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>0.346895</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.185203</td>\n",
       "      <td>0.328872</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.177465</td>\n",
       "      <td>0.325344</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.718681</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.170260</td>\n",
       "      <td>0.337473</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.163104</td>\n",
       "      <td>0.345836</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.734634</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.156267</td>\n",
       "      <td>0.392615</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.719093</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.149450</td>\n",
       "      <td>0.283797</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.812637</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>0.304804</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.139605</td>\n",
       "      <td>0.342894</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.734115</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.134922</td>\n",
       "      <td>0.361508</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.765281</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.130852</td>\n",
       "      <td>0.311926</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.733985</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.125493</td>\n",
       "      <td>0.297191</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.702690</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.121169</td>\n",
       "      <td>0.290705</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.702690</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>0.281763</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.765281</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.112563</td>\n",
       "      <td>0.286026</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.108850</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.765396</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.104898</td>\n",
       "      <td>0.291826</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.781197</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.101957</td>\n",
       "      <td>0.292631</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.765510</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.099501</td>\n",
       "      <td>0.292795</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.749817</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.096745</td>\n",
       "      <td>0.292841</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.749817</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
